{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4289f1b1",
   "metadata": {},
   "source": [
    "# TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c60209",
   "metadata": {},
   "source": [
    "# 1.Describe the purpose and benefits of pooling in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd8cb0",
   "metadata": {},
   "source": [
    "Pooling is a crucial operation in Convolutional Neural Networks (CNNs) that plays a significant role in reducing the spatial dimensions of the input data while retaining important features. The primary purpose of pooling is to downsample the feature maps, making subsequent computations more efficient and reducing the risk of overfitting. There are two common types of pooling used in CNNs: Max Pooling and Average Pooling.\n",
    "\n",
    "1. **Spatial Downsampling:**\n",
    "   - **Purpose:** Pooling helps in reducing the spatial dimensions (width and height) of the input volume, effectively decreasing the amount of computation in the network. This is particularly important in CNNs, where the size of the input data can be large (e.g., images).\n",
    "   - **Benefits:** It reduces the computational complexity and memory requirements, making the network more computationally efficient. Smaller feature maps also help in capturing more prominent patterns and generalizing better.\n",
    "\n",
    "2. **Feature Invariance:**\n",
    "   - **Purpose:** Pooling contributes to achieving translation invariance or shift invariance in the learned features. This means that the network becomes less sensitive to the exact position of features within the receptive field.\n",
    "   - **Benefits:** By extracting the most salient information from a local region, pooling helps in making the network more robust to variations in the position of features. This is especially useful in tasks such as image recognition, where the exact location of a feature may not be critical.\n",
    "\n",
    "3. **Reduction of Overfitting:**\n",
    "   - **Purpose:** Pooling reduces the spatial dimensions and, consequently, the number of parameters in the network. This helps in preventing overfitting by introducing a form of regularization.\n",
    "   - **Benefits:** With fewer parameters, the network is less likely to memorize the training data and is more likely to generalize well to unseen data.\n",
    "\n",
    "4. **Computationally Efficient:**\n",
    "   - **Purpose:** Pooling reduces the number of computations in subsequent layers, making the network more computationally efficient.\n",
    "   - **Benefits:** As the size of the feature maps decreases, the computational cost of the network decreases, allowing for faster training and inference.\n",
    "\n",
    "5. **Downsampling for Hierarchical Feature Extraction:**\n",
    "   - **Purpose:** Pooling facilitates a hierarchical representation of features. As the network progresses through layers, pooling helps in capturing more abstract and high-level features.\n",
    "   - **Benefits:** By gradually reducing spatial dimensions, the network can focus on learning increasingly complex and abstract representations, leading to a more hierarchical and discriminative feature hierarchy.\n",
    "\n",
    "Pooling in CNNs serves the purpose of spatial downsampling, feature invariance, reducing overfitting, computational efficiency, and facilitating hierarchical feature extraction. These benefits collectively contribute to the effectiveness of CNNs in tasks such as image recognition, object detection, and other computer vision applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58c070",
   "metadata": {},
   "source": [
    "# 2. Explain the difference between min pooling and max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c927c1",
   "metadata": {},
   "source": [
    "Min pooling and max pooling are both types of pooling operations commonly used in Convolutional Neural Networks (CNNs) for downsampling feature maps. The key difference between them lies in how they aggregate information within the pooling window.\n",
    "\n",
    "1. **Max Pooling:**\n",
    "   - **Operation:** In max pooling, for each local region (pooling window) in the input feature map, the maximum value within that region is retained, and the rest are discarded.\n",
    "   - **Purpose:** Max pooling is effective in capturing the most prominent features within a local region. It helps in retaining the most important information and is particularly useful when the goal is to focus on the most activated features.\n",
    "\n",
    "2. **Min Pooling:**\n",
    "   - **Operation:** In min pooling, for each local region (pooling window) in the input feature map, the minimum value within that region is retained, and the rest are discarded.\n",
    "   - **Purpose:** Min pooling is effective in capturing the least intense features within a local region. It helps in preserving information about the least activated features and is useful when the goal is to focus on the least prominent aspects.\n",
    "\n",
    "**Comparison:**\n",
    "- Max pooling tends to capture the most significant features and is often preferred in tasks where the precise localization of features is important, such as object recognition.\n",
    "- Min pooling can be useful in scenarios where the presence of the least intense features is more critical, and it can be applied in certain specialized contexts.\n",
    "- Both max pooling and min pooling contribute to downsampling the spatial dimensions of the feature maps, aiding in computational efficiency and reducing the risk of overfitting.\n",
    "- In practice, max pooling is more commonly used, but the choice between max pooling and min pooling depends on the specific requirements of the task at hand. Sometimes, average pooling is also used, where the average value within the pooling window is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ab1ff1",
   "metadata": {},
   "source": [
    "# 3. Discuss the concept of padding in CNN and its significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b5b28e",
   "metadata": {},
   "source": [
    "Padding is a technique used in Convolutional Neural Networks (CNNs) to add extra pixels or values around the input data before performing convolution or pooling operations. The primary purpose of padding is to control the spatial dimensions of the feature maps throughout the network and address some of the limitations associated with convolutional and pooling layers. Padding is usually applied to input data before a convolutional layer or pooling layer, and it involves adding zeros or other constant values around the input.\n",
    "\n",
    "Here are key aspects and significance of padding in CNNs:\n",
    "\n",
    "1. **Preventing Spatial Dimension Reduction:**\n",
    "   - **Significance:** Convolutional and pooling operations, especially those with a filter or pooling window larger than 1x1, can lead to a reduction in spatial dimensions (width and height) of the feature maps. Padding helps mitigate this reduction, allowing the network to retain more spatial information.\n",
    "   - **Effect:** Without padding, as convolutional and pooling layers are applied successively, the spatial dimensions of the feature maps tend to decrease rapidly, potentially resulting in loss of spatial information.\n",
    "\n",
    "2. **Preserving Information at the Edges:**\n",
    "   - **Significance:** Convolution operations can be problematic near the edges of an image. Without padding, only a fraction of the filter overlaps with the pixels at the image border, leading to the loss of information.\n",
    "   - **Effect:** Padding ensures that the entire filter can be applied to pixels at the image border, preserving information at the edges and preventing the network from neglecting valuable details.\n",
    "\n",
    "3. **Mitigating Border Effects:**\n",
    "   - **Significance:** In the absence of padding, the pixels at the border of the feature maps are involved in fewer convolutional or pooling operations compared to the central pixels. This can result in a border effect, where the importance of features near the borders is underestimated.\n",
    "   - **Effect:** Padding helps mitigate border effects by ensuring that the convolutional and pooling operations are applied uniformly across the input data, allowing the network to treat all pixels equally.\n",
    "\n",
    "4. **Controlling Output Size:**\n",
    "   - **Significance:** Padding allows for better control over the output size of the feature maps after convolution or pooling operations.\n",
    "   - **Effect:** By adjusting the amount of padding, one can influence the spatial dimensions of the output feature maps. This control is valuable for designing neural networks with specific architectural requirements or when aligning input and output dimensions in a network.\n",
    "\n",
    "5. **Compatibility with Strided Convolution:**\n",
    "   - **Significance:** Strided convolution involves skipping pixels during the convolution operation. Padding is useful in conjunction with strided convolution to maintain a desired spatial resolution in the feature maps.\n",
    "   - **Effect:** Padding ensures that the spatial resolution reduction caused by strided convolution is balanced by the addition of extra pixels, allowing for more flexibility in designing architectures.\n",
    "\n",
    "padding in CNNs is a crucial technique that addresses issues related to spatial dimension reduction, preserves information at the edges, mitigates border effects, and provides control over the output size. It plays a key role in maintaining spatial information throughout the network and contributes to the overall effectiveness of convolutional neural networks in tasks such as image recognition and computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef005ff4",
   "metadata": {},
   "source": [
    "# 4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be147de6",
   "metadata": {},
   "source": [
    "Zero-padding and valid-padding are two common types of padding techniques used in Convolutional Neural Networks (CNNs) to control the spatial dimensions of the output feature maps after convolution or pooling operations. Let's compare and contrast these two padding strategies in terms of their effects on the output feature map size:\n",
    "\n",
    "1. **Zero-padding:**\n",
    "   - **Operation:** In zero-padding, additional rows and columns of zeros are added around the input data before applying convolution or pooling operations. The zeros act as a border around the input.\n",
    "   - **Effect on Output Size:** Zero-padding increases the spatial dimensions of the input feature map, effectively preventing a reduction in size caused by subsequent convolution or pooling layers.\n",
    "   - **Formula for Output Size:** If `P` is the amount of zero-padding, `F` is the size of the filter (kernel), `S` is the stride, and `W_in` is the input width or height, the output size (`W_out`) can be calculated using the formula:\n",
    "     ```\n",
    "     W_out = (W_in - F + 2P) / S + 1\n",
    "     ```\n",
    "\n",
    "2. **Valid-padding:**\n",
    "   - **Operation:** In valid-padding (also known as \"no-padding\" or \"without padding\"), no additional rows or columns are added around the input data. The convolution or pooling operation is applied directly to the input, and the filter is only placed where it entirely fits within the input dimensions.\n",
    "   - **Effect on Output Size:** Valid-padding may lead to a reduction in the spatial dimensions of the feature map, especially if the filter size or the pooling window size is larger than 1x1.\n",
    "   - **Formula for Output Size:** For valid-padding, the output size (`W_out`) is calculated using the formula:\n",
    "     ```\n",
    "     W_out = (W_in - F) / S + 1\n",
    "     ```\n",
    "\n",
    "**Comparison:**\n",
    "- **Output Size Preservation:**\n",
    "  - **Zero-padding:** Preserves the spatial dimensions by adding zeros around the input, preventing a reduction in size.\n",
    "  - **Valid-padding:** May lead to a reduction in spatial dimensions, especially with larger filter sizes or pooling windows.\n",
    "\n",
    "- **Control over Output Size:**\n",
    "  - **Zero-padding:** Provides explicit control over the output size by adjusting the amount of padding.\n",
    "  - **Valid-padding:** Output size is determined solely by the filter size, stride, and input size.\n",
    "\n",
    "- **Border Effects:**\n",
    "  - **Zero-padding:** Helps mitigate border effects by ensuring that the entire filter is applied to pixels at the image border.\n",
    "  - **Valid-padding:** May lead to border effects as pixels near the edges are involved in fewer operations.\n",
    "\n",
    "- **Edge Preservation:**\n",
    "  - **Zero-padding:** Preserves information at the edges by including zeros around the input.\n",
    "  - **Valid-padding:** Information at the edges may be lost if the filter extends beyond the input boundaries.\n",
    "\n",
    "zero-padding and valid-padding have contrasting effects on the output feature map size. Zero-padding is useful for preserving spatial information, mitigating border effects, and providing control over output size, while valid-padding may result in a reduction in size and border effects, making it suitable for cases where downsampling is intentional or desired. The choice between these padding strategies depends on the specific requirements of the neural network architecture and the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00099d48",
   "metadata": {},
   "source": [
    "# TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d062020",
   "metadata": {},
   "source": [
    "# 1. Provide a brief overview of LeNet-5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ca7e9",
   "metadata": {},
   "source": [
    "LeNet-5 is a pioneering Convolutional Neural Network (CNN) architecture designed for handwritten digit recognition, particularly for the task of recognizing characters in postal codes on checks during the 1990s. It was developed by Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. LeNet-5 played a significant role in demonstrating the effectiveness of CNNs for image recognition tasks and laid the groundwork for subsequent developments in deep learning.\n",
    "\n",
    "Here is a brief overview of the LeNet-5 architecture:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - LeNet-5 takes grayscale images of size 32x32 as input.\n",
    "\n",
    "2. **First Convolutional Layer (C1):**\n",
    "   - Convolutional operation with a 5x5 kernel.\n",
    "   - Output feature maps have 6 channels.\n",
    "   - Sigmoid activation function is applied.\n",
    "   - Subsampling (average pooling) is performed with a 2x2 window and a stride of 2.\n",
    "\n",
    "3. **Second Convolutional Layer (C3):**\n",
    "   - Convolutional operation with a 5x5 kernel.\n",
    "   - Output feature maps have 16 channels.\n",
    "   - Sigmoid activation function is applied.\n",
    "   - Subsampling (average pooling) is performed with a 2x2 window and a stride of 2.\n",
    "\n",
    "4. **Third Convolutional Layer (C5):**\n",
    "   - Convolutional operation with a 5x5 kernel.\n",
    "   - Output feature maps have 120 channels.\n",
    "   - Sigmoid activation function is applied.\n",
    "\n",
    "5. **Fully Connected Layers:**\n",
    "   - Two fully connected layers follow the convolutional layers.\n",
    "   - The first fully connected layer (F6) has 84 neurons and a sigmoid activation function.\n",
    "   - The second fully connected layer (output layer) has 10 neurons (corresponding to the 10 digits) with a softmax activation function for classification.\n",
    "\n",
    "6. **Activation Functions:**\n",
    "   - Sigmoid activation functions are used throughout the network, except for the output layer, which employs softmax.\n",
    "\n",
    "7. **Training:**\n",
    "   - LeNet-5 was trained using the gradient-based optimization algorithm, specifically the stochastic gradient descent (SGD) method.\n",
    "\n",
    "8. **Loss Function:**\n",
    "   - Cross-entropy loss was commonly used as the loss function during training.\n",
    "\n",
    "9. **Overall Structure:**\n",
    "   - LeNet-5 has a hierarchical and modular architecture, with convolutional and subsampling layers followed by fully connected layers.\n",
    "\n",
    "10. **Innovations:**\n",
    "    - LeNet-5 introduced the concept of using convolutional layers with shared weights and biases, which significantly reduced the number of parameters compared to fully connected networks.\n",
    "    - The architecture demonstrated the importance of local receptive fields, weight sharing, and subsampling for efficient feature learning in image data.\n",
    "\n",
    "While LeNet-5 was initially designed for digit recognition, its principles have influenced the development of more advanced CNN architectures used in various computer vision tasks, including image classification, object detection, and segmentation. Despite its relatively simple structure by today's standards, LeNet-5 laid the groundwork for the widespread adoption of deep learning in computer vision applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90632e62",
   "metadata": {},
   "source": [
    "# 2. Describe the key components of LeNet-5 and their respective purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1bde9",
   "metadata": {},
   "source": [
    "LeNet-5 consists of several key components, each serving a specific purpose in the architecture. Here are the key components of LeNet-5 and their respective purposes:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - **Purpose:** The input layer takes grayscale images of size 32x32 pixels. Each pixel represents the intensity of the grayscale value. The input layer provides the raw image data to the subsequent layers for feature extraction.\n",
    "\n",
    "2. **First Convolutional Layer (C1):**\n",
    "   - **Purpose:** The first convolutional layer applies convolutional operations to the input image. It uses a 5x5 kernel to convolve with the input, generating feature maps. The purpose is to capture basic patterns and features in the input image, such as edges and simple textures.\n",
    "\n",
    "3. **Subsampling (S2):**\n",
    "   - **Purpose:** Subsampling, often performed through average pooling, reduces the spatial dimensions of the feature maps from the previous convolutional layer. It helps in retaining important information while reducing the computational complexity. In LeNet-5, S2 uses a 2x2 pooling window with a stride of 2.\n",
    "\n",
    "4. **Second Convolutional Layer (C3):**\n",
    "   - **Purpose:** Similar to the first convolutional layer, C3 applies convolutional operations with a 5x5 kernel to the subsampled feature maps from S2. This layer captures more complex patterns and features compared to C1.\n",
    "\n",
    "5. **Subsampling (S4):**\n",
    "   - **Purpose:** Subsampling is applied again to the feature maps from C3 using average pooling. S4 reduces the spatial dimensions further while preserving important information. It helps create invariance to small translations and distortions.\n",
    "\n",
    "6. **Third Convolutional Layer (C5):**\n",
    "   - **Purpose:** C5 is another convolutional layer that operates on the subsampled feature maps from S4. It captures even higher-level features and prepares the network for the fully connected layers.\n",
    "\n",
    "7. **Fully Connected Layers (F6 and Output Layer):**\n",
    "   - **Purpose:** Following the convolutional layers, LeNet-5 includes two fully connected layers. F6 is the first fully connected layer with 84 neurons, and the output layer is the final layer with 10 neurons, each corresponding to a digit (0-9). These layers perform high-level feature combination and classification.\n",
    "\n",
    "8. **Activation Functions (Sigmoid and Softmax):**\n",
    "   - **Purpose:** Sigmoid activation functions are applied in the convolutional and fully connected layers, introducing non-linearity to the network. The final layer uses softmax activation to convert the network's output into probabilities for each digit class, facilitating multi-class classification.\n",
    "\n",
    "9. **Innovative Concepts:**\n",
    "   - **Shared Weights and Biases:** LeNet-5 introduced the concept of shared weights and biases in convolutional layers, which reduces the number of parameters and allows the network to learn spatial hierarchies of features more efficiently.\n",
    "   - **Local Receptive Fields:** The network employs small, localized receptive fields, which helps capture spatial hierarchies of features and makes the network invariant to small translations.\n",
    "\n",
    "10. **Loss Function and Training:**\n",
    "    - **Purpose:** LeNet-5 uses a cross-entropy loss function for training through stochastic gradient descent (SGD). The training process aims to minimize the difference between the predicted probabilities and the actual class labels.\n",
    "\n",
    "LeNet-5's key components include convolutional layers for feature extraction, subsampling layers for dimensionality reduction, fully connected layers for high-level feature combination, and activation functions for introducing non-linearity. The innovative concepts of shared weights, local receptive fields, and specific architectural choices contribute to its effectiveness in image recognition tasks, particularly handwritten digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2cfc3",
   "metadata": {},
   "source": [
    "# 3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b132ed62",
   "metadata": {},
   "source": [
    "LeNet-5, as one of the pioneering Convolutional Neural Network (CNN) architectures, had several advantages and limitations, particularly in the context of image classification tasks. While it played a significant role in demonstrating the effectiveness of CNNs for this purpose, advancements in deep learning have led to more sophisticated architectures. Here's an overview of the advantages and limitations of LeNet-5:\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. **Effective Feature Hierarchy:**\n",
    "   - LeNet-5 introduced the concept of using shared weights and biases in convolutional layers, allowing the network to learn spatial hierarchies of features effectively. This contributes to the extraction of hierarchical and complex features from images.\n",
    "\n",
    "2. **Localized Receptive Fields:**\n",
    "   - The use of small, localized receptive fields helps capture local features and spatial hierarchies. This is particularly beneficial for image recognition tasks where recognizing local patterns is crucial.\n",
    "\n",
    "3. **Translation Invariance:**\n",
    "   - The architecture includes subsampling layers that provide translation invariance, making the network robust to small translations and distortions in the input data. This is advantageous for recognizing patterns in different positions within an image.\n",
    "\n",
    "4. **Applicability to Handwritten Digit Recognition:**\n",
    "   - LeNet-5 was originally designed for handwritten digit recognition, specifically for recognizing characters in postal codes on checks. It performed well in this context and demonstrated the applicability of CNNs to similar tasks.\n",
    "\n",
    "5. **Architectural Simplicity:**\n",
    "   - LeNet-5 has a relatively simple architecture compared to more recent CNNs. This simplicity can be advantageous in scenarios with limited computational resources, making it easier to deploy on devices with lower processing capabilities.\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "1. **Limited Capacity for Complex Tasks:**\n",
    "   - LeNet-5 may be insufficient for more complex image classification tasks, such as recognizing objects in diverse scenes. Its architecture is relatively shallow compared to modern CNNs designed for more challenging datasets.\n",
    "\n",
    "2. **Sigmoid Activation Functions:**\n",
    "   - The use of sigmoid activation functions in LeNet-5 introduces some limitations, such as the vanishing gradient problem. Modern architectures often use rectified linear units (ReLU) to address this issue and accelerate convergence during training.\n",
    "\n",
    "3. **Small Input Size:**\n",
    "   - LeNet-5 was designed for small 32x32 input images, limiting its applicability to tasks requiring higher-resolution input data. In contemporary image classification tasks, larger input sizes are common, allowing networks to capture more detailed features.\n",
    "\n",
    "4. **Not Suitable for Large and Diverse Datasets:**\n",
    "   - While effective for its original purpose of digit recognition, LeNet-5 may not perform optimally on large and diverse datasets, such as those used in current benchmarks like ImageNet. More recent architectures with deeper layers have shown better performance on such datasets.\n",
    "\n",
    "5. **Limited Exploration of Activation Functions and Regularization:**\n",
    "   - LeNet-5 primarily used sigmoid activation functions and did not extensively explore modern activation functions or regularization techniques. This limits its ability to benefit from advancements that improve the training and generalization capabilities of neural networks.\n",
    "\n",
    "while LeNet-5 demonstrated the potential of CNNs for image classification tasks and remains influential, its limitations in handling larger and more diverse datasets have led to the development of more advanced architectures. Modern CNNs, such as VGG, ResNet, and EfficientNet, have since addressed some of these limitations, leading to improved performance on a wide range of image recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ea5d7",
   "metadata": {},
   "source": [
    "# 4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffb815d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\getan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\getan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         456       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 14, 14, 6)         0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 5, 5, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62006 (242.21 KB)\n",
      "Trainable params: 62006 (242.21 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\getan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\getan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "391/391 [==============================] - 5s 8ms/step - loss: 1.8494 - accuracy: 0.3423 - val_loss: 1.7178 - val_accuracy: 0.3990\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 3s 7ms/step - loss: 1.6830 - accuracy: 0.4056 - val_loss: 1.6146 - val_accuracy: 0.4307\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.6146 - accuracy: 0.4307\n",
      "Test Loss: 1.614646077156067\n",
      "Test accuracy: 0.43070000410079956\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "# Building the Model Architecture\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(6, kernel_size = (5,5), padding = 'valid', activation='tanh', input_shape = (32,32,3)))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Conv2D(16, kernel_size = (5,5), padding = 'valid', activation='tanh'))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=2, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b998a42",
   "metadata": {},
   "source": [
    "# TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef861fc3",
   "metadata": {},
   "source": [
    "# 1. Present an overview of the AlexNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c941cf1",
   "metadata": {},
   "source": [
    "AlexNet is a pioneering deep convolutional neural network (CNN) architecture that significantly contributed to the advancement of image classification tasks. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, demonstrating the effectiveness of deep learning in computer vision. Here's an overview of the AlexNet architecture:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - AlexNet takes as input color images with a resolution of 224x224 pixels.\n",
    "\n",
    "2. **Convolutional Layers (Conv1, Conv2, Conv3, Conv4, Conv5):**\n",
    "   - The network consists of five convolutional layers, each followed by a Rectified Linear Unit (ReLU) activation function.\n",
    "   - The first convolutional layer (Conv1) uses a large 11x11 filter with a stride of 4.\n",
    "   - Subsequent convolutional layers use smaller filters (3x3 or 5x5) with a stride of 1.\n",
    "   - Conv1 and Conv2 are followed by max-pooling layers with a 3x3 window and a stride of 2.\n",
    "   - Conv3, Conv4, and Conv5 do not have explicit pooling layers.\n",
    "\n",
    "3. **Normalization (Norm1, Norm2):**\n",
    "   - Local Response Normalization (LRN) is applied after the first and second convolutional layers.\n",
    "   - LRN normalizes the responses across neighboring channels, enhancing the network's ability to generalize.\n",
    "\n",
    "4. **Dropout:**\n",
    "   - Dropout is applied after the first two fully connected layers (F6 and F7).\n",
    "   - Dropout helps prevent overfitting by randomly dropping a fraction of the neurons during training.\n",
    "\n",
    "5. **Fully Connected Layers (F6, F7, F8, Output Layer):**\n",
    "   - The architecture includes three fully connected layers and an output layer.\n",
    "   - F6 and F7 have 4096 neurons each, followed by ReLU activation functions.\n",
    "   - F8 (output layer) has 1000 neurons, corresponding to the number of ImageNet classes, and employs a softmax activation function for classification.\n",
    "\n",
    "6. **Softmax Output:**\n",
    "   - The softmax function is applied to the output layer to convert the network's raw predictions into class probabilities.\n",
    "   - The class with the highest probability is considered the final prediction.\n",
    "\n",
    "7. **Architecture Innovations:**\n",
    "   - **Use of Rectified Linear Units (ReLU):** AlexNet popularized the use of ReLU activation functions, which help mitigate the vanishing gradient problem and accelerate convergence during training.\n",
    "   - **Data Augmentation:** Data augmentation techniques, such as random cropping and horizontal flipping, were employed during training to increase the diversity of the training set.\n",
    "   - **Overlapping Pooling:** Overlapping max-pooling layers were used to reduce the spatial dimensions of the feature maps while preserving more information compared to non-overlapping pooling.\n",
    "\n",
    "8. **Parallel Processing:**\n",
    "   - AlexNet was designed to leverage the computational power of GPUs. It utilized two GPUs for parallel processing, with each processing different portions of the neural network architecture.\n",
    "\n",
    "AlexNet demonstrated the effectiveness of deep CNNs for image classification tasks and set the stage for subsequent advancements in deep learning. Its architectural innovations, including the use of ReLU activation functions, data augmentation, and overlapping pooling, have influenced the design of many subsequent CNN architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863603b7",
   "metadata": {},
   "source": [
    "# 2. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough performancem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55703180",
   "metadata": {},
   "source": [
    "AlexNet introduced several architectural innovations that played a crucial role in its breakthrough performance and subsequent success in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. These innovations addressed key challenges in training deep neural networks and significantly improved the accuracy of image classification tasks. Here are the main architectural innovations in AlexNet:\n",
    "\n",
    "1. **ReLU Activation Function:**\n",
    "   - **Innovation:** AlexNet was one of the first CNN architectures to extensively use Rectified Linear Units (ReLU) as activation functions.\n",
    "   - **Significance:** ReLU introduces non-linearity to the network and helps mitigate the vanishing gradient problem. Compared to traditional activation functions like sigmoid or hyperbolic tangent, ReLU accelerates the convergence of training by allowing for faster and more effective learning.\n",
    "\n",
    "2. **Local Response Normalization (LRN):**\n",
    "   - **Innovation:** AlexNet incorporated Local Response Normalization (LRN) after the first and second convolutional layers.\n",
    "   - **Significance:** LRN normalizes the responses across neighboring channels. This helps enhance the network's ability to generalize by promoting competition among neurons, giving a boost to the most strongly activated neurons and suppressing others. LRN was introduced as a form of lateral inhibition to encourage diversity in feature learning.\n",
    "\n",
    "3. **Overlapping Max-Pooling:**\n",
    "   - **Innovation:** AlexNet used overlapping max-pooling layers.\n",
    "   - **Significance:** Overlapping pooling was employed to reduce the spatial dimensions of the feature maps while preserving more information compared to non-overlapping pooling. This was done by using a stride smaller than the pooling window size, allowing some overlap between adjacent pooling regions. Overlapping pooling helps maintain more spatial information and contributes to better translation invariance.\n",
    "\n",
    "4. **Data Augmentation:**\n",
    "   - **Innovation:** AlexNet applied data augmentation during training.\n",
    "   - **Significance:** Data augmentation involves applying random transformations to the input data during training, such as random cropping, horizontal flipping, and color adjustments. This technique increases the diversity of the training set, making the model more robust to variations in the input data. Data augmentation is especially important when dealing with limited training data, as it helps prevent overfitting.\n",
    "\n",
    "5. **Dropout Regularization:**\n",
    "   - **Innovation:** Dropout regularization was applied after the first two fully connected layers (F6 and F7).\n",
    "   - **Significance:** Dropout involves randomly dropping a fraction of the neurons during training. This regularization technique helps prevent overfitting by reducing the reliance on specific neurons, forcing the network to learn more robust features. It introduces a form of ensemble learning within a single model.\n",
    "\n",
    "6. **Parallel Processing with Two GPUs:**\n",
    "   - **Innovation:** AlexNet was designed for parallel processing using two GPUs.\n",
    "   - **Significance:** The use of two GPUs enabled faster training and improved model convergence. Each GPU processed different portions of the neural network architecture in parallel, allowing for efficient utilization of computational resources. This parallel processing contributed to the scalability of deep learning models.\n",
    "\n",
    "These architectural innovations collectively contributed to the breakthrough performance of AlexNet in the ILSVRC 2012 competition. The success of AlexNet demonstrated the potential of deep learning for image classification tasks and paved the way for the development of more advanced CNN architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a3103",
   "metadata": {},
   "source": [
    "# 3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e10918c",
   "metadata": {},
   "source": [
    "In the AlexNet architecture, convolutional layers, pooling layers, and fully connected layers play distinct but complementary roles in the overall process of feature extraction, dimensionality reduction, and classification. Here's a discussion of the roles of each layer type in AlexNet:\n",
    "\n",
    "1. **Convolutional Layers:**\n",
    "   - **Role:** Convolutional layers are responsible for learning spatial hierarchies of features from the input images. They apply convolutional operations with learnable filters (kernels) to capture local patterns and features in different regions of the input.\n",
    "   - **Significance in AlexNet:**\n",
    "     - The first convolutional layer (Conv1) in AlexNet uses a large 11x11 filter with a stride of 4. This allows the network to capture large and coarse features in the input images.\n",
    "     - Subsequent convolutional layers (Conv2 to Conv5) use smaller filter sizes (3x3 or 5x5) with a stride of 1 to capture more fine-grained features.\n",
    "     - Convolutional layers are followed by ReLU activation functions, introducing non-linearity to the network.\n",
    "\n",
    "2. **Pooling Layers:**\n",
    "   - **Role:** Pooling layers are used to reduce the spatial dimensions of the feature maps generated by the convolutional layers. Pooling is a form of subsampling that retains the most important information while reducing computational complexity.\n",
    "   - **Significance in AlexNet:**\n",
    "     - AlexNet employs both max-pooling and overlapping max-pooling layers.\n",
    "     - The max-pooling layers follow the first and second convolutional layers (Conv1 and Conv2) with a 3x3 window and a stride of 2.\n",
    "     - Overlapping pooling is used to maintain more spatial information compared to non-overlapping pooling. This helps preserve translation invariance.\n",
    "\n",
    "3. **Fully Connected Layers:**\n",
    "   - **Role:** Fully connected layers are responsible for combining high-level features extracted by the convolutional layers and making final predictions. They map the learned features to class probabilities in the case of classification tasks.\n",
    "   - **Significance in AlexNet:**\n",
    "     - AlexNet has three fully connected layers: F6, F7, and F8 (output layer).\n",
    "     - F6 and F7 each have 4096 neurons with ReLU activation functions. These layers serve as feature combination stages.\n",
    "     - F8, the output layer, has 1000 neurons corresponding to the number of classes in the ImageNet dataset. It employs a softmax activation function to produce class probabilities for classification.\n",
    "\n",
    "4. **Normalization Layers (LRN):**\n",
    "   - **Role:** Local Response Normalization (LRN) is applied after the first and second convolutional layers in AlexNet.\n",
    "   - **Significance in AlexNet:**\n",
    "     - LRN normalizes the responses across neighboring channels, enhancing the network's ability to generalize by promoting competition among neurons.\n",
    "     - LRN was introduced to provide a form of lateral inhibition, encouraging diversity in feature learning.\n",
    "\n",
    "5. **Dropout Regularization:**\n",
    "   - **Role:** Dropout is applied after the first two fully connected layers (F6 and F7).\n",
    "   - **Significance in AlexNet:**\n",
    "     - Dropout helps prevent overfitting by randomly dropping a fraction of the neurons during training. It reduces the network's reliance on specific neurons and forces it to learn more robust features.\n",
    "\n",
    "Convolutional layers are responsible for feature extraction, pooling layers reduce spatial dimensions while preserving important information, and fully connected layers combine high-level features for classification. The specific design choices, such as the use of ReLU activation, LRN, overlapping pooling, and dropout, were instrumental in the success of AlexNet in the ImageNet competition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1ddcb",
   "metadata": {},
   "source": [
    "# 4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c258e4",
   "metadata": {},
   "source": [
    "Implementing AlexNet from scratch is a complex task due to its architectural details, and it may require significant computational resources for training. However, I can provide you with a simplified implementation using a deep learning framework such as TensorFlow or PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b60dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\getan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 6, 6, 96)          34944     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 2, 2, 96)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 2, 2, 256)         614656    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 1, 1, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 1, 1, 384)         885120    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 1, 1, 384)         1327488   \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 1, 1, 256)         884992    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 1, 1, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4096)              1052672   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21622154 (82.48 MB)\n",
      "Trainable params: 21622154 (82.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\getan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training_v1.py:635: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - ETA: 0s - loss: 2.0224 - acc: 0.2027"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\getan\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 38s 750us/sample - loss: 2.0224 - acc: 0.2027 - val_loss: 1.9144 - val_acc: 0.2371\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 45s 894us/sample - loss: 1.8245 - acc: 0.2861 - val_loss: 1.7696 - val_acc: 0.3102\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 46s 928us/sample - loss: 1.7390 - acc: 0.3286 - val_loss: 1.7057 - val_acc: 0.3379\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 47s 933us/sample - loss: 1.6836 - acc: 0.3600 - val_loss: 1.7433 - val_acc: 0.3403\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 48s 956us/sample - loss: 1.6367 - acc: 0.3875 - val_loss: 1.6163 - val_acc: 0.3995\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 47s 945us/sample - loss: 1.5853 - acc: 0.4106 - val_loss: 1.6192 - val_acc: 0.3972\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 47s 941us/sample - loss: 1.5331 - acc: 0.4368 - val_loss: 1.5558 - val_acc: 0.4329\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 49s 972us/sample - loss: 1.4905 - acc: 0.4565 - val_loss: 1.5038 - val_acc: 0.4522\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 48s 969us/sample - loss: 1.4491 - acc: 0.4735 - val_loss: 1.4925 - val_acc: 0.4660\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 49s 985us/sample - loss: 1.4128 - acc: 0.4916 - val_loss: 1.4765 - val_acc: 0.4651\n",
      "Test accuracy: 0.4650999903678894\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define AlexNet architecture\n",
    "model = models.Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((3, 3), strides=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(layers.Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "# Layer 3\n",
    "model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# Layer 4\n",
    "model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "# Layer 5\n",
    "model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "\n",
    "# Flatten and fully connected layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f3ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
